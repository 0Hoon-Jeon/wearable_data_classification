{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/jyh/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ubuntu/anaconda3/envs/jyh/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ubuntu/anaconda3/envs/jyh/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ubuntu/anaconda3/envs/jyh/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ubuntu/anaconda3/envs/jyh/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ubuntu/anaconda3/envs/jyh/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-63b2f08dc98e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'model'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "from scipy import signal\n",
    "import random \n",
    "from tensorflow import keras \n",
    "from sklearn.preprocessing import MinMaxScaler,OneHotEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import multi_gpu_model\n",
    "from tensorflow.keras.layers import Reshape,CuDNNLSTM,LeakyReLU,ZeroPadding1D,Conv1D,BatchNormalization,Activation,MaxPooling1D,Dropout,Dense,Flatten,Bidirectional,LSTM,GlobalAvgPool1D\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler,EarlyStopping,ReduceLROnPlateau\n",
    "import itertools\n",
    "\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S001 = severity : 0, E1 len: 4 ,\tE2 len: 4 ,\tE3 len: 8 ,\tE4 len: 5 ,\t\n",
      "S002 = severity : 0, E1 len: 2 ,\tE2 len: 4 ,\tE3 len: 8 ,\tE4 len: 0 ,\t\n",
      "S003 = severity : 0, E1 len: 2 ,\tE2 len: 2 ,\tE3 len: 6 ,\tE4 len: 0 ,\t\n",
      "S004 = severity : 0, E1 len: 2 ,\tE2 len: 4 ,\tE3 len: 14 ,\tE4 len: 0 ,\t\n",
      "S005 = severity : 0, E1 len: 2 ,\tE2 len: 2 ,\tE3 len: 9 ,\tE4 len: 6 ,\t\n",
      "S006 = severity : 3, E1 len: 4 ,\tE2 len: 2 ,\tE3 len: 5 ,\tE4 len: 6 ,\t\n",
      "S007 = severity : 1, E1 len: 2 ,\tE2 len: 6 ,\tE3 len: 6 ,\tE4 len: 11 ,\t\n",
      "S008 = severity : 0, E1 len: 2 ,\tE2 len: 2 ,\tE3 len: 5 ,\tE4 len: 4 ,\t\n",
      "S009 = severity : 0, E1 len: 2 ,\tE2 len: 2 ,\tE3 len: 7 ,\tE4 len: 5 ,\t\n",
      "S010 = severity : 0, E1 len: 3 ,\tE2 len: 3 ,\tE3 len: 6 ,\tE4 len: 6 ,\t\n",
      "S011 = severity : 0, E1 len: 2 ,\tE2 len: 2 ,\tE3 len: 5 ,\tE4 len: 4 ,\t\n",
      "S012 = severity : 5, E1 len: 2 ,\tE2 len: 2 ,\tE3 len: 6 ,\tE4 len: 5 ,\t\n",
      "S013 = severity : 5, E1 len: 2 ,\tE2 len: 2 ,\tE3 len: 8 ,\tE4 len: 5 ,\t\n",
      "S014 = severity : 5, E1 len: 4 ,\tE2 len: 4 ,\tE3 len: 15 ,\tE4 len: 11 ,\t\n",
      "S015 = severity : 4, E1 len: 5 ,\tE2 len: 2 ,\tE3 len: 0 ,\tE4 len: 0 ,\t\n",
      "S016 = severity : 3, E1 len: 2 ,\tE2 len: 2 ,\tE3 len: 5 ,\tE4 len: 4 ,\t\n",
      "S017 = severity : 0, E1 len: 3 ,\tE2 len: 2 ,\tE3 len: 6 ,\tE4 len: 3 ,\t\n",
      "S018 = severity : 0, E1 len: 3 ,\tE2 len: 2 ,\tE3 len: 6 ,\tE4 len: 3 ,\t\n",
      "S019 = severity : 0, E1 len: 2 ,\tE2 len: 2 ,\tE3 len: 6 ,\tE4 len: 3 ,\t\n",
      "S020 = severity : 0, E1 len: 2 ,\tE2 len: 2 ,\tE3 len: 6 ,\tE4 len: 3 ,\t\n",
      "S021 = severity : 0, E1 len: 2 ,\tE2 len: 2 ,\tE3 len: 6 ,\tE4 len: 3 ,\t\n",
      "S022 = severity : 3, E1 len: 4 ,\tE2 len: 2 ,\tE3 len: 9 ,\tE4 len: 3 ,\t\n",
      "S023 = severity : 1, E1 len: 4 ,\tE2 len: 2 ,\tE3 len: 12 ,\tE4 len: 9 ,\t\n",
      "S024 = severity : 0, E1 len: 4 ,\tE2 len: 4 ,\tE3 len: 10 ,\tE4 len: 10 ,\t\n",
      "S025 = severity : 5, E1 len: 5 ,\tE2 len: 4 ,\tE3 len: 11 ,\tE4 len: 7 ,\t\n",
      "S026 = severity : 0, E1 len: 3 ,\tE2 len: 2 ,\tE3 len: 6 ,\tE4 len: 4 ,\t\n",
      "S027 = severity : 0, E1 len: 3 ,\tE2 len: 2 ,\tE3 len: 5 ,\tE4 len: 3 ,\t\n",
      "S028 = severity : 0, E1 len: 4 ,\tE2 len: 5 ,\tE3 len: 13 ,\tE4 len: 8 ,\t\n",
      "S029 = severity : 1, E1 len: 4 ,\tE2 len: 5 ,\tE3 len: 12 ,\tE4 len: 7 ,\t\n",
      "S030 = severity : 0, E1 len: 5 ,\tE2 len: 4 ,\tE3 len: 10 ,\tE4 len: 8 ,\t\n",
      "S031 = severity : 0, E1 len: 4 ,\tE2 len: 4 ,\tE3 len: 12 ,\tE4 len: 6 ,\t\n",
      "S032 = severity : 1, E1 len: 2 ,\tE2 len: 2 ,\tE3 len: 5 ,\tE4 len: 3 ,\t\n",
      "S033 = severity : 1, E1 len: 2 ,\tE2 len: 2 ,\tE3 len: 5 ,\tE4 len: 3 ,\t\n",
      "S034 = severity : 1, E1 len: 7 ,\tE2 len: 4 ,\tE3 len: 14 ,\tE4 len: 8 ,\t\n",
      "S035 = severity : 1, E1 len: 4 ,\tE2 len: 4 ,\tE3 len: 12 ,\tE4 len: 6 ,\t\n",
      "S036 = severity : 0, E1 len: 2 ,\tE2 len: 2 ,\tE3 len: 6 ,\tE4 len: 3 ,\t\n",
      "S037 = severity : 1, E1 len: 5 ,\tE2 len: 4 ,\tE3 len: 13 ,\tE4 len: 8 ,\t\n",
      "S038 = severity : 1, E1 len: 2 ,\tE2 len: 2 ,\tE3 len: 6 ,\tE4 len: 5 ,\t\n",
      "S039 = severity : 0, E1 len: 4 ,\tE2 len: 13 ,\tE3 len: 6 ,\tE4 len: 11 ,\t\n",
      "S040 = severity : 1, E1 len: 4 ,\tE2 len: 4 ,\tE3 len: 12 ,\tE4 len: 7 ,\t\n",
      "S041 = severity : 1, E1 len: 2 ,\tE2 len: 6 ,\tE3 len: 13 ,\tE4 len: 7 ,\t\n",
      "S042 = severity : 1, E1 len: 4 ,\tE2 len: 5 ,\tE3 len: 13 ,\tE4 len: 9 ,\t\n",
      "S043 = severity : 1, E1 len: 4 ,\tE2 len: 4 ,\tE3 len: 12 ,\tE4 len: 7 ,\t\n",
      "S044 = severity : 3, E1 len: 2 ,\tE2 len: 4 ,\tE3 len: 8 ,\tE4 len: 2 ,\t\n",
      "S045 = severity : 1, E1 len: 4 ,\tE2 len: 5 ,\tE3 len: 9 ,\tE4 len: 7 ,\t\n",
      "S046 = severity : 1, E1 len: 4 ,\tE2 len: 4 ,\tE3 len: 10 ,\tE4 len: 7 ,\t\n",
      "S047 = severity : 0, E1 len: 4 ,\tE2 len: 4 ,\tE3 len: 14 ,\tE4 len: 8 ,\t\n",
      "S048 = severity : 0, E1 len: 2 ,\tE2 len: 2 ,\tE3 len: 6 ,\tE4 len: 3 ,\t\n",
      "S049 = severity : 0, E1 len: 4 ,\tE2 len: 5 ,\tE3 len: 10 ,\tE4 len: 9 ,\t\n",
      "S050 = severity : 0, E1 len: 4 ,\tE2 len: 0 ,\tE3 len: 7 ,\tE4 len: 3 ,\t\n",
      "S051 = severity : 0, E1 len: 2 ,\tE2 len: 2 ,\tE3 len: 5 ,\tE4 len: 4 ,\t\n",
      "S052 = severity : 3, E1 len: 4 ,\tE2 len: 6 ,\tE3 len: 9 ,\tE4 len: 8 ,\t\n",
      "S053 = severity : 0, E1 len: 2 ,\tE2 len: 2 ,\tE3 len: 6 ,\tE4 len: 3 ,\t\n",
      "S054 = severity : 0, E1 len: 2 ,\tE2 len: 2 ,\tE3 len: 6 ,\tE4 len: 3 ,\t\n",
      "S055 = severity : 0, E1 len: 2 ,\tE2 len: 2 ,\tE3 len: 5 ,\tE4 len: 3 ,\t\n",
      "S056 = severity : 0, E1 len: 2 ,\tE2 len: 3 ,\tE3 len: 6 ,\tE4 len: 5 ,\t\n",
      "S057 = severity : 0, E1 len: 4 ,\tE2 len: 4 ,\tE3 len: 13 ,\tE4 len: 5 ,\t\n",
      "S058 = severity : 0, E1 len: 6 ,\tE2 len: 4 ,\tE3 len: 11 ,\tE4 len: 6 ,\t\n",
      "S059 = severity : 0, E1 len: 4 ,\tE2 len: 4 ,\tE3 len: 10 ,\tE4 len: 6 ,\t\n",
      "S060 = severity : 1, E1 len: 7 ,\tE2 len: 5 ,\tE3 len: 12 ,\tE4 len: 6 ,\t\n",
      "S061 = severity : 1, E1 len: 2 ,\tE2 len: 2 ,\tE3 len: 5 ,\tE4 len: 3 ,\t\n",
      "S062 = severity : 0, E1 len: 3 ,\tE2 len: 4 ,\tE3 len: 11 ,\tE4 len: 6 ,\t\n",
      "S063 = severity : 1, E1 len: 6 ,\tE2 len: 4 ,\tE3 len: 15 ,\tE4 len: 6 ,\t\n",
      "S064 = severity : 1, E1 len: 4 ,\tE2 len: 5 ,\tE3 len: 10 ,\tE4 len: 8 ,\t\n",
      "S065 = severity : 1, E1 len: 3 ,\tE2 len: 2 ,\tE3 len: 5 ,\tE4 len: 3 ,\t\n",
      "S066 = severity : 3, E1 len: 10 ,\tE2 len: 2 ,\tE3 len: 5 ,\tE4 len: 4 ,\t\n",
      "S067 = severity : 3, E1 len: 2 ,\tE2 len: 1 ,\tE3 len: 5 ,\tE4 len: 3 ,\t\n",
      "S068 = severity : 1, E1 len: 9 ,\tE2 len: 0 ,\tE3 len: 0 ,\tE4 len: 4 ,\t\n",
      "S069 = severity : 0, E1 len: 2 ,\tE2 len: 2 ,\tE3 len: 6 ,\tE4 len: 3 ,\t\n",
      "S070 = severity : 0, E1 len: 5 ,\tE2 len: 4 ,\tE3 len: 11 ,\tE4 len: 7 ,\t\n",
      "S071 = severity : 6, E1 len: 2 ,\tE2 len: 2 ,\tE3 len: 5 ,\tE4 len: 3 ,\t\n",
      "S072 = severity : 0, E1 len: 2 ,\tE2 len: 2 ,\tE3 len: 5 ,\tE4 len: 5 ,\t\n",
      "S073 = severity : 1, E1 len: 2 ,\tE2 len: 2 ,\tE3 len: 6 ,\tE4 len: 4 ,\t\n",
      "S074 = severity : 6, E1 len: 2 ,\tE2 len: 2 ,\tE3 len: 5 ,\tE4 len: 3 ,\t\n",
      "S075 = severity : 1, E1 len: 4 ,\tE2 len: 4 ,\tE3 len: 11 ,\tE4 len: 8 ,\t\n",
      "S076 = severity : 0, E1 len: 4 ,\tE2 len: 4 ,\tE3 len: 12 ,\tE4 len: 7 ,\t\n",
      "S077 = severity : 1, E1 len: 4 ,\tE2 len: 4 ,\tE3 len: 16 ,\tE4 len: 6 ,\t\n",
      "S078 = severity : 1, E1 len: 4 ,\tE2 len: 4 ,\tE3 len: 14 ,\tE4 len: 9 ,\t\n",
      "S079 = severity : 1, E1 len: 4 ,\tE2 len: 4 ,\tE3 len: 13 ,\tE4 len: 6 ,\t\n",
      "S080 = severity : 1, E1 len: 4 ,\tE2 len: 4 ,\tE3 len: 10 ,\tE4 len: 6 ,\t\n",
      "S081 = severity : 0, E1 len: 4 ,\tE2 len: 4 ,\tE3 len: 10 ,\tE4 len: 6 ,\t\n",
      "S082 = severity : 1, E1 len: 4 ,\tE2 len: 4 ,\tE3 len: 10 ,\tE4 len: 7 ,\t\n",
      "S083 = severity : 0, E1 len: 2 ,\tE2 len: 3 ,\tE3 len: 5 ,\tE4 len: 3 ,\t\n",
      "S084 = severity : 1, E1 len: 2 ,\tE2 len: 2 ,\tE3 len: 5 ,\tE4 len: 3 ,\t\n",
      "S085 = severity : 0, E1 len: 2 ,\tE2 len: 2 ,\tE3 len: 6 ,\tE4 len: 3 ,\t\n",
      "S086 = severity : 0, E1 len: 2 ,\tE2 len: 2 ,\tE3 len: 6 ,\tE4 len: 3 ,\t\n",
      "S087 = severity : 0, E1 len: 2 ,\tE2 len: 2 ,\tE3 len: 6 ,\tE4 len: 3 ,\t\n",
      "S088 = severity : 0, E1 len: 2 ,\tE2 len: 2 ,\tE3 len: 5 ,\tE4 len: 3 ,\t\n",
      "S089 = severity : 0, E1 len: 2 ,\tE2 len: 2 ,\tE3 len: 6 ,\tE4 len: 4 ,\t\n",
      "S090 = severity : 0, E1 len: 2 ,\tE2 len: 2 ,\tE3 len: 6 ,\tE4 len: 3 ,\t\n",
      "S091 = severity : 0, E1 len: 2 ,\tE2 len: 2 ,\tE3 len: 5 ,\tE4 len: 3 ,\t\n",
      "S092 = severity : 0, E1 len: 2 ,\tE2 len: 2 ,\tE3 len: 7 ,\tE4 len: 3 ,\t\n",
      "S093 = severity : 0, E1 len: 2 ,\tE2 len: 2 ,\tE3 len: 6 ,\tE4 len: 3 ,\t\n",
      "S094 = severity : 0, E1 len: 2 ,\tE2 len: 2 ,\tE3 len: 7 ,\tE4 len: 3 ,\t\n",
      "S095 = severity : 0, E1 len: 2 ,\tE2 len: 2 ,\tE3 len: 5 ,\tE4 len: 3 ,\t\n",
      "S096 = severity : 0, E1 len: 3 ,\tE2 len: 2 ,\tE3 len: 5 ,\tE4 len: 3 ,\t\n",
      "S097 = severity : 0, E1 len: 2 ,\tE2 len: 2 ,\tE3 len: 5 ,\tE4 len: 3 ,\t\n",
      "S098 = severity : 1, E1 len: 4 ,\tE2 len: 5 ,\tE3 len: 17 ,\tE4 len: 11 ,\t\n",
      "S099 = severity : 0, E1 len: 2 ,\tE2 len: 2 ,\tE3 len: 6 ,\tE4 len: 8 ,\t\n",
      "S100 = severity : 0, E1 len: 2 ,\tE2 len: 2 ,\tE3 len: 6 ,\tE4 len: 5 ,\t\n",
      "S101 = severity : 0, E1 len: 2 ,\tE2 len: 2 ,\tE3 len: 5 ,\tE4 len: 6 ,\t\n",
      "S102 = severity : 0, E1 len: 2 ,\tE2 len: 2 ,\tE3 len: 5 ,\tE4 len: 3 ,\t\n",
      "S103 = severity : 0, E1 len: 2 ,\tE2 len: 2 ,\tE3 len: 6 ,\tE4 len: 3 ,\t\n",
      "S104 = severity : 0, E1 len: 2 ,\tE2 len: 2 ,\tE3 len: 6 ,\tE4 len: 4 ,\t\n",
      "S105 = severity : 0, E1 len: 2 ,\tE2 len: 2 ,\tE3 len: 7 ,\tE4 len: 4 ,\t\n",
      "S106 = severity : 3, E1 len: 3 ,\tE2 len: 8 ,\tE3 len: 8 ,\tE4 len: 5 ,\t\n",
      "S107 = severity : 3, E1 len: 3 ,\tE2 len: 4 ,\tE3 len: 14 ,\tE4 len: 12 ,\t\n",
      "S108 = severity : 0, E1 len: 3 ,\tE2 len: 2 ,\tE3 len: 8 ,\tE4 len: 6 ,\t\n",
      "S109 = severity : 0, E1 len: 2 ,\tE2 len: 2 ,\tE3 len: 5 ,\tE4 len: 5 ,\t\n",
      "S110 = severity : 0, E1 len: 4 ,\tE2 len: 3 ,\tE3 len: 9 ,\tE4 len: 3 ,\t\n",
      "S111 = severity : 1, E1 len: 2 ,\tE2 len: 2 ,\tE3 len: 5 ,\tE4 len: 3 ,\t\n",
      "S112 = severity : 1, E1 len: 2 ,\tE2 len: 3 ,\tE3 len: 9 ,\tE4 len: 3 ,\t\n",
      "S113 = severity : 0, E1 len: 4 ,\tE2 len: 4 ,\tE3 len: 10 ,\tE4 len: 6 ,\t\n",
      "S114 = severity : 1, E1 len: 4 ,\tE2 len: 5 ,\tE3 len: 12 ,\tE4 len: 9 ,\t\n",
      "S115 = severity : 1, E1 len: 3 ,\tE2 len: 3 ,\tE3 len: 8 ,\tE4 len: 5 ,\t\n",
      "S116 = severity : 1, E1 len: 2 ,\tE2 len: 2 ,\tE3 len: 7 ,\tE4 len: 3 ,\t\n",
      "S117 = severity : 1, E1 len: 4 ,\tE2 len: 6 ,\tE3 len: 13 ,\tE4 len: 10 ,\t\n",
      "S118 = severity : 0, E1 len: 4 ,\tE2 len: 4 ,\tE3 len: 10 ,\tE4 len: 7 ,\t\n",
      "S119 = severity : 0, E1 len: 3 ,\tE2 len: 2 ,\tE3 len: 5 ,\tE4 len: 3 ,\t\n",
      "S120 = severity : 0, E1 len: 2 ,\tE2 len: 2 ,\tE3 len: 5 ,\tE4 len: 3 ,\t\n",
      "S121 = severity : 0, E1 len: 2 ,\tE2 len: 2 ,\tE3 len: 5 ,\tE4 len: 3 ,\t\n",
      "S122 = severity : 0, E1 len: 2 ,\tE2 len: 2 ,\tE3 len: 5 ,\tE4 len: 3 ,\t\n",
      "S123 = severity : 0, E1 len: 2 ,\tE2 len: 2 ,\tE3 len: 5 ,\tE4 len: 3 ,\t\n",
      "S124 = severity : 0, E1 len: 2 ,\tE2 len: 2 ,\tE3 len: 7 ,\tE4 len: 3 ,\t\n",
      "S125 = severity : 0, E1 len: 2 ,\tE2 len: 2 ,\tE3 len: 7 ,\tE4 len: 3 ,\t\n",
      "S126 = severity : 0, E1 len: 2 ,\tE2 len: 2 ,\tE3 len: 6 ,\tE4 len: 4 ,\t\n",
      "S127 = severity : 0, E1 len: 2 ,\tE2 len: 2 ,\tE3 len: 6 ,\tE4 len: 3 ,\t\n",
      "S128 = severity : 0, E1 len: 2 ,\tE2 len: 2 ,\tE3 len: 5 ,\tE4 len: 3 ,\t\n",
      "S129 = severity : 0, E1 len: 2 ,\tE2 len: 2 ,\tE3 len: 5 ,\tE4 len: 7 ,\t\n",
      "S130 = severity : 0, E1 len: 2 ,\tE2 len: 2 ,\tE3 len: 8 ,\tE4 len: 4 ,\t\n",
      "S131 = severity : 0, E1 len: 2 ,\tE2 len: 4 ,\tE3 len: 5 ,\tE4 len: 3 ,\t\n",
      "S132 = severity : 0, E1 len: 4 ,\tE2 len: 2 ,\tE3 len: 6 ,\tE4 len: 4 ,\t\n",
      "S133 = severity : 1, E1 len: 4 ,\tE2 len: 4 ,\tE3 len: 11 ,\tE4 len: 12 ,\t\n",
      "S134 = severity : 1, E1 len: 2 ,\tE2 len: 2 ,\tE3 len: 7 ,\tE4 len: 4 ,\t\n",
      "S135 = severity : 0, E1 len: 2 ,\tE2 len: 2 ,\tE3 len: 5 ,\tE4 len: 4 ,\t\n",
      "S136 = severity : 1, E1 len: 4 ,\tE2 len: 4 ,\tE3 len: 10 ,\tE4 len: 7 ,\t\n",
      "S137 = severity : 1, E1 len: 4 ,\tE2 len: 4 ,\tE3 len: 11 ,\tE4 len: 7 ,\t\n",
      "S138 = severity : 1, E1 len: 2 ,\tE2 len: 2 ,\tE3 len: 6 ,\tE4 len: 3 ,\t\n",
      "S139 = severity : 1, E1 len: 3 ,\tE2 len: 2 ,\tE3 len: 7 ,\tE4 len: 3 ,\t\n",
      "S140 = severity : 1, E1 len: 4 ,\tE2 len: 8 ,\tE3 len: 22 ,\tE4 len: 10 ,\t\n",
      "S141 = severity : 1, E1 len: 2 ,\tE2 len: 6 ,\tE3 len: 12 ,\tE4 len: 7 ,\t\n",
      "S142 = severity : 3, E1 len: 3 ,\tE2 len: 5 ,\tE3 len: 13 ,\tE4 len: 8 ,\t\n",
      "S143 = severity : 1, E1 len: 2 ,\tE2 len: 3 ,\tE3 len: 7 ,\tE4 len: 3 ,\t\n",
      "S144 = severity : 1, E1 len: 3 ,\tE2 len: 2 ,\tE3 len: 10 ,\tE4 len: 4 ,\t\n",
      "S145 = severity : 1, E1 len: 2 ,\tE2 len: 2 ,\tE3 len: 7 ,\tE4 len: 4 ,\t\n",
      "S146 = severity : 1, E1 len: 5 ,\tE2 len: 5 ,\tE3 len: 10 ,\tE4 len: 7 ,\t\n",
      "S147 = severity : 0, E1 len: 2 ,\tE2 len: 2 ,\tE3 len: 5 ,\tE4 len: 3 ,\t\n",
      "S148 = severity : 1, E1 len: 2 ,\tE2 len: 2 ,\tE3 len: 5 ,\tE4 len: 3 ,\t\n",
      "S149 = severity : 1, E1 len: 2 ,\tE2 len: 5 ,\tE3 len: 8 ,\tE4 len: 5 ,\t\n",
      "S150 = severity : 1, E1 len: 2 ,\tE2 len: 2 ,\tE3 len: 5 ,\tE4 len: 3 ,\t\n",
      "S151 = severity : 1, E1 len: 2 ,\tE2 len: 2 ,\tE3 len: 5 ,\tE4 len: 3 ,\t\n",
      "S152 = severity : 0, E1 len: 4 ,\tE2 len: 4 ,\tE3 len: 10 ,\tE4 len: 6 ,\t\n",
      "S153 = severity : 1, E1 len: 2 ,\tE2 len: 2 ,\tE3 len: 5 ,\tE4 len: 3 ,\t\n",
      "S154 = severity : 1, E1 len: 2 ,\tE2 len: 2 ,\tE3 len: 5 ,\tE4 len: 4 ,\t\n",
      "S155 = severity : 0, E1 len: 2 ,\tE2 len: 3 ,\tE3 len: 5 ,\tE4 len: 3 ,\t\n",
      "S156 = severity : 3, E1 len: 1 ,\tE2 len: 4 ,\tE3 len: 2 ,\tE4 len: 2 ,\t\n",
      "S157 = severity : 0, E1 len: 2 ,\tE2 len: 2 ,\tE3 len: 8 ,\tE4 len: 3 ,\t\n",
      "S158 = severity : 1, E1 len: 2 ,\tE2 len: 2 ,\tE3 len: 6 ,\tE4 len: 3 ,\t\n",
      "S159 = severity : 0, E1 len: 2 ,\tE2 len: 2 ,\tE3 len: 9 ,\tE4 len: 3 ,\t\n",
      "S160 = severity : 1, E1 len: 2 ,\tE2 len: 2 ,\tE3 len: 5 ,\tE4 len: 3 ,\t\n",
      "S161 = severity : 1, E1 len: 2 ,\tE2 len: 2 ,\tE3 len: 5 ,\tE4 len: 3 ,\t\n",
      "S162 = severity : 1, E1 len: 2 ,\tE2 len: 2 ,\tE3 len: 5 ,\tE4 len: 3 ,\t\n",
      "S163 = severity : 1, E1 len: 2 ,\tE2 len: 2 ,\tE3 len: 5 ,\tE4 len: 3 ,\t\n",
      "S164 = severity : 1, E1 len: 2 ,\tE2 len: 2 ,\tE3 len: 7 ,\tE4 len: 3 ,\t\n",
      "S165 = severity : 1, E1 len: 2 ,\tE2 len: 2 ,\tE3 len: 5 ,\tE4 len: 3 ,\t\n",
      "S166 = severity : 3, E1 len: 1 ,\tE2 len: 4 ,\tE3 len: 0 ,\tE4 len: 1 ,\t\n",
      "S167 = severity : 0, E1 len: 2 ,\tE2 len: 1 ,\tE3 len: 5 ,\tE4 len: 3 ,\t\n",
      "S168 = severity : 1, E1 len: 2 ,\tE2 len: 2 ,\tE3 len: 5 ,\tE4 len: 4 ,\t\n",
      "S169 = severity : 3, E1 len: 1 ,\tE2 len: 1 ,\tE3 len: 2 ,\tE4 len: 1 ,\t\n",
      "S170 = severity : 0, E1 len: 2 ,\tE2 len: 2 ,\tE3 len: 5 ,\tE4 len: 3 ,\t\n",
      "S171 = severity : 0, E1 len: 2 ,\tE2 len: 2 ,\tE3 len: 5 ,\tE4 len: 3 ,\t\n",
      "S172 = severity : 0, E1 len: 2 ,\tE2 len: 2 ,\tE3 len: 5 ,\tE4 len: 3 ,\t\n",
      "S173 = severity : 1, E1 len: 2 ,\tE2 len: 3 ,\tE3 len: 5 ,\tE4 len: 4 ,\t\n",
      "S174 = severity : 1, E1 len: 2 ,\tE2 len: 2 ,\tE3 len: 5 ,\tE4 len: 3 ,\t\n",
      "S175 = severity : 1, E1 len: 2 ,\tE2 len: 2 ,\tE3 len: 5 ,\tE4 len: 3 ,\t\n",
      "S176 = severity : 1, E1 len: 2 ,\tE2 len: 2 ,\tE3 len: 5 ,\tE4 len: 3 ,\t\n",
      "S177 = severity : 0, E1 len: 2 ,\tE2 len: 2 ,\tE3 len: 5 ,\tE4 len: 3 ,\t\n",
      "S178 = severity : 0, E1 len: 2 ,\tE2 len: 2 ,\tE3 len: 5 ,\tE4 len: 3 ,\t\n",
      "S179 = severity : 3, E1 len: 2 ,\tE2 len: 0 ,\tE3 len: 2 ,\tE4 len: 1 ,\t\n",
      "S180 = severity : 3, E1 len: 1 ,\tE2 len: 2 ,\tE3 len: 1 ,\tE4 len: 1 ,\t\n",
      "S181 = severity : 4, E1 len: 1 ,\tE2 len: 1 ,\tE3 len: 2 ,\tE4 len: 1 ,\t\n",
      "S182 = severity : 3, E1 len: 1 ,\tE2 len: 3 ,\tE3 len: 1 ,\tE4 len: 1 ,\t\n",
      "S183 = severity : 0, E1 len: 2 ,\tE2 len: 3 ,\tE3 len: 5 ,\tE4 len: 3 ,\t\n",
      "S184 = severity : 0, E1 len: 2 ,\tE2 len: 2 ,\tE3 len: 5 ,\tE4 len: 3 ,\t\n",
      "S185 = severity : 4, E1 len: 0 ,\tE2 len: 0 ,\tE3 len: 0 ,\tE4 len: 0 ,\t\n",
      "S186 = severity : 3, E1 len: 1 ,\tE2 len: 2 ,\tE3 len: 1 ,\tE4 len: 1 ,\t\n",
      "S187 = severity : 1, E1 len: 2 ,\tE2 len: 2 ,\tE3 len: 5 ,\tE4 len: 3 ,\t\n",
      "S188 = severity : 3, E1 len: 1 ,\tE2 len: 2 ,\tE3 len: 1 ,\tE4 len: 1 ,\t\n",
      "S189 = severity : 3, E1 len: 2 ,\tE2 len: 2 ,\tE3 len: 2 ,\tE4 len: 1 ,\t\n",
      "S190 = severity : 1, E1 len: 2 ,\tE2 len: 2 ,\tE3 len: 8 ,\tE4 len: 3 ,\t\n",
      "S191 = severity : 0, E1 len: 2 ,\tE2 len: 3 ,\tE3 len: 5 ,\tE4 len: 3 ,\t\n",
      "S192 = severity : 1, E1 len: 2 ,\tE2 len: 2 ,\tE3 len: 5 ,\tE4 len: 3 ,\t\n",
      "S193 = severity : 3, E1 len: 1 ,\tE2 len: 1 ,\tE3 len: 3 ,\tE4 len: 1 ,\t\n",
      "S194 = severity : 3, E1 len: 1 ,\tE2 len: 1 ,\tE3 len: 1 ,\tE4 len: 1 ,\t\n",
      "\n",
      "S total : \n",
      "E1 : 95 ,\tE2 : 69 ,\tE3 : 0 ,\tE4 : 21 ,\tE5 : 3 ,\tE6 : 4 ,\tE7 : 2 ,\t\n",
      "E total : \n",
      "E1 : 529 ,\tE2 : 545 ,\tE3 : 1390 ,\tE4 : 859 ,\t"
     ]
    }
   ],
   "source": [
    "prep_path = '/home/ubuntu/Dataset/prep_path_pulse1/'\n",
    "label_csv = '/home/ubuntu/Dataset/현황_간소화.xlsx'\n",
    "model_path = '/home/ubuntu/Dataset/model/'\n",
    "seed = 7\n",
    "\n",
    "total_len = 194\n",
    "\n",
    "def load_label():\n",
    "    label_filename = label_csv\n",
    "    label_df = pd.read_excel(label_filename,header=2).drop([0]) # remove first line(figure)\n",
    "    label_df = label_df[['실험NO.','중증도\\n5단계']]\n",
    "    label_df.columns=['subjectnb','label']\n",
    "    label_df.replace({'label':{'Normal':0,'Presymptomatic AD':0,'Prodroaml AD':1,'AD Dementia 초기 ':3,'AD Dementia 초기':3,'AD Dementia 중기 ':4,'AD Dementia중기 ':4,'MCI unlikely due to AD':5,'PET or CSF (x)':6}},inplace=True)\n",
    "    label_df.set_index(['subjectnb'],inplace=True)\n",
    "    \n",
    "    return dict(label_df.label)\n",
    "\n",
    "\n",
    "label_dict = load_label()\n",
    "\n",
    "class Subject():\n",
    "    def __init__(self,snum,label_dict):\n",
    "        if type(snum) is int:\n",
    "            self.snum = 'S{:03}'.format(snum)\n",
    "        elif type(snum) is str:\n",
    "            self.snum = snum\n",
    "        self.path = prep_path+self.snum\n",
    "        self.data_dict = dict()\n",
    "        \n",
    "        for i in range(1,5):\n",
    "            self.data_dict['E{}'.format(i)] = os.listdir(self.path+'/E{}'.format(i))\n",
    "            self.data_dict['E{}'.format(i)].sort()\n",
    "        \n",
    "        self.severity = label_dict[self.snum]\n",
    "    def load_data(self,filename):\n",
    "        data = np.load(filename)\n",
    "        return data\n",
    "    \n",
    "    def get_snum(self):\n",
    "        return self.snum\n",
    "    def get_severity(self):\n",
    "        return self.severity\n",
    "    \n",
    "    def get_filenames(self,enum):\n",
    "        return self.data_dict['E{}'.format(enum)]\n",
    "    \n",
    "    def get_filenames_path(self,enum):\n",
    "        res = []\n",
    "        file_path = self.path+'/E{}'.format(enum)\n",
    "        for filename in self.data_dict['E{}'.format(enum)] :\n",
    "            res.append(file_path+'/'+filename)\n",
    "        return res\n",
    "    \n",
    "    def get_data(self,enum):\n",
    "        res = []\n",
    "        filenames = self.get_filenames_path(enum)\n",
    "        \n",
    "        for filename in filenames:\n",
    "            res.append(self.load_data(filename))\n",
    "        \n",
    "        return res\n",
    "    \n",
    "    def get_len(self):\n",
    "        res = []\n",
    "        for i in self.data_dict:\n",
    "            res.append(len(self.data_dict[i]))\n",
    "        \n",
    "        return res\n",
    "    \n",
    "    def print_info(self):\n",
    "        print('{} = severity : {}, '.format(self.snum,self.severity),end='')\n",
    "        for i,num in enumerate(self.get_len()):\n",
    "            print('E{} len: {} '.format(i+1,num),end = ',\\t')\n",
    "        print()\n",
    "\n",
    "S_cnt =  [[] for x in range(7) ]\n",
    "\n",
    "E_cnt = np.zeros(4,dtype=int)\n",
    "\n",
    "for i in range(1,total_len+1):\n",
    "    tmp = Subject(i,label_dict)\n",
    "    tmp.print_info()\n",
    "    S_cnt[tmp.get_severity()].append(tmp.get_snum()) \n",
    "    E_cnt += tmp.get_len()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "print()\n",
    "print('S total : ',)\n",
    "for i,s in enumerate(S_cnt):\n",
    "    print('E{} : {} '.format(i+1,len(s)),end = ',\\t')\n",
    "print()   \n",
    "print('E total : ')\n",
    "for i,e in enumerate(E_cnt):\n",
    "    print('E{} : {} '.format(i+1,e),end = ',\\t')\n",
    "\n",
    "def subject_split(S_cnt,using_num=[0,1],seed = random.randint(1,100),print_list=False):\n",
    "    train,val,test = [],[],[]\n",
    "    \n",
    "    for i in using_num:\n",
    "        train_list,remain_list = train_test_split(S_cnt[i],test_size=0.3, random_state=seed)\n",
    "        val_list,test_list = train_test_split(remain_list,test_size=0.7, random_state=seed)\n",
    "        print('label {} split result = train : {} , val : {} , test : {} : '.format(i,len(train_list),len(val_list),len(test_list)))\n",
    "        train.extend(train_list)\n",
    "        val.extend(val_list)\n",
    "        test.extend(test_list)\n",
    "    \n",
    "    train.sort()\n",
    "    val.sort()\n",
    "    test.sort()\n",
    "    print('total split result = train : {} , val : {} , test : {}'.format(len(train),len(val),len(test)))\n",
    "    \n",
    "    if print_list:\n",
    "        print('\\ntrain_list\\n', train)\n",
    "        print('\\nval_list\\n', val)\n",
    "        print('\\ntest_list\\n', test)\n",
    "        \n",
    "    return train,val,test\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def preprocessing(train_list,val_list,test_list,e_num=1,is_scale=False,resampling_len = 32*32):\n",
    "    X_train,y_train = [],[]\n",
    "    X_val,y_val = [],[]\n",
    "    X_test,y_test = [],[]\n",
    "    \n",
    "    \n",
    "    label_dict = load_label()\n",
    "    \n",
    "    print('train_list data load')\n",
    "    for i in train_list:\n",
    "#         print(i)\n",
    "        \n",
    "        tmp = Subject(i,label_dict)\n",
    "        label = tmp.get_severity()\n",
    "        data = tmp.get_data(e_num)\n",
    "        \n",
    "        for j in data:\n",
    "            tmp_x = np.zeros((32,32, 63)).astype(np.float32)\n",
    "            for k in range(63):\n",
    "                resample = signal.resample(j[:,k],resampling_len)\n",
    "                \n",
    "                if is_scale:\n",
    "                    \n",
    "                    scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "                    resample = scaler.fit_transform(resample.reshape(-1,1))\n",
    "                \n",
    "                tmp_x[:,:,k] = np.reshape(resample,(32,32))\n",
    "                \n",
    "            \n",
    "            \n",
    "#             plt.figure(figsize=(15,15))\n",
    "#             for k in range(63):\n",
    "#                 plt.subplot(7,9,k+1)\n",
    "#                 plt.plot(tmp_x[:,k])\n",
    "#             return\n",
    "            X_train.append(tmp_x)\n",
    "            y_train.append(label)\n",
    "        \n",
    "    print('val_list data load')\n",
    "    print(val_list)\n",
    "    for i in val_list:\n",
    "#         print(i)\n",
    "        \n",
    "        tmp = Subject(i,label_dict)\n",
    "        label = tmp.get_severity()\n",
    "        data = tmp.get_data(e_num)\n",
    "        for j in data:\n",
    "            tmp_x = np.zeros((32,32, 63)).astype(np.float32)\n",
    "            for k in range(63):\n",
    "#                 tmp_x[:,k] = signal.resample(j[:,k],resampling_len)\n",
    "                resample = signal.resample(j[:,k],resampling_len)\n",
    "    \n",
    "                if is_scale:\n",
    "                    \n",
    "                    scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "                    resample = scaler.fit_transform(resample.reshape(-1,1))\n",
    "    \n",
    "                tmp_x[:,:,k] = np.reshape(resample,(32,32))\n",
    "                \n",
    "            \n",
    "                \n",
    "                \n",
    "            \n",
    "            X_val.append(tmp_x)\n",
    "            y_val.append(label)\n",
    "            \n",
    "    print('test_list data load')\n",
    "    print(test_list)\n",
    "\n",
    "    for i in test_list:\n",
    "#         print(i)\n",
    "        \n",
    "        tmp = Subject(i,label_dict)\n",
    "        label = tmp.get_severity()\n",
    "        data = tmp.get_data(e_num)\n",
    "        for j in data:\n",
    "            tmp_x = np.zeros((32,32, 63)).astype(np.float32)\n",
    "            for k in range(63):\n",
    "                resample = signal.resample(j[:,k],resampling_len)\n",
    "                \n",
    "                if is_scale:\n",
    "                    \n",
    "                    scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "                    resample = scaler.fit_transform(resample.reshape(-1,1))\n",
    "                \n",
    "                tmp_x[:,:,k] = np.reshape(resample,(32,32))\n",
    "#                 tmp_x[:,k] = signal.resample(j[:,k],resampling_len)\n",
    "                \n",
    "#             plt.figure(figsize=(15,15))\n",
    "#             for k in range(63):\n",
    "#                 plt.subplot(7,9,k+1)\n",
    "#                 plt.plot(tmp_x[:,k])\n",
    "#             return\n",
    "            X_test.append(tmp_x)\n",
    "            y_test.append(label)\n",
    "            \n",
    "            \n",
    "    enc = OneHotEncoder()\n",
    "    \n",
    "    X_train = np.array(X_train)\n",
    "    \n",
    "    y_train = np.reshape(y_train,(-1,1))\n",
    "    y_train = enc.fit_transform(y_train).toarray()\n",
    "    \n",
    "    X_val = np.array(X_val)\n",
    "    \n",
    "    y_val = np.reshape(y_val,(-1,1))\n",
    "    y_val = enc.fit_transform(y_val).toarray()\n",
    "    \n",
    "    X_test = np.array(X_test)\n",
    "    \n",
    "    y_test = np.reshape(y_test,(-1,1))\n",
    "    y_test = enc.fit_transform(y_test).toarray()\n",
    "    \n",
    "    return X_train,y_train,X_val,y_val,X_test,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label 0 split result = train : 66 , val : 8 , test : 21 : \n",
      "label 1 split result = train : 48 , val : 6 , test : 15 : \n",
      "total split result = train : 114 , val : 14 , test : 36\n",
      "\n",
      "train_list\n",
      " ['S001', 'S002', 'S004', 'S005', 'S007', 'S009', 'S010', 'S011', 'S017', 'S018', 'S023', 'S024', 'S027', 'S031', 'S032', 'S033', 'S034', 'S035', 'S037', 'S038', 'S039', 'S040', 'S041', 'S042', 'S043', 'S046', 'S048', 'S049', 'S050', 'S053', 'S054', 'S055', 'S057', 'S058', 'S061', 'S062', 'S065', 'S069', 'S075', 'S076', 'S077', 'S078', 'S079', 'S080', 'S081', 'S085', 'S086', 'S087', 'S088', 'S089', 'S090', 'S091', 'S092', 'S093', 'S096', 'S097', 'S099', 'S100', 'S101', 'S102', 'S105', 'S108', 'S110', 'S112', 'S116', 'S117', 'S118', 'S120', 'S121', 'S122', 'S123', 'S125', 'S126', 'S128', 'S129', 'S130', 'S131', 'S132', 'S133', 'S134', 'S135', 'S136', 'S137', 'S139', 'S140', 'S141', 'S143', 'S145', 'S146', 'S147', 'S148', 'S150', 'S151', 'S153', 'S155', 'S157', 'S158', 'S159', 'S160', 'S162', 'S164', 'S167', 'S170', 'S172', 'S173', 'S174', 'S175', 'S176', 'S177', 'S184', 'S187', 'S190', 'S191', 'S192']\n",
      "\n",
      "val_list\n",
      " ['S070', 'S072', 'S082', 'S083', 'S084', 'S103', 'S104', 'S111', 'S119', 'S144', 'S152', 'S161', 'S163', 'S183']\n",
      "\n",
      "test_list\n",
      " ['S003', 'S008', 'S019', 'S020', 'S021', 'S026', 'S028', 'S029', 'S030', 'S036', 'S045', 'S047', 'S051', 'S056', 'S059', 'S060', 'S063', 'S064', 'S068', 'S073', 'S094', 'S095', 'S098', 'S109', 'S113', 'S114', 'S115', 'S124', 'S127', 'S138', 'S149', 'S154', 'S165', 'S168', 'S171', 'S178']\n",
      "train_list data load\n",
      "val_list data load\n",
      "['S070', 'S072', 'S082', 'S083', 'S084', 'S103', 'S104', 'S111', 'S119', 'S144', 'S152', 'S161', 'S163', 'S183']\n",
      "test_list data load\n",
      "['S003', 'S008', 'S019', 'S020', 'S021', 'S026', 'S028', 'S029', 'S030', 'S036', 'S045', 'S047', 'S051', 'S056', 'S059', 'S060', 'S063', 'S064', 'S068', 'S073', 'S094', 'S095', 'S098', 'S109', 'S113', 'S114', 'S115', 'S124', 'S127', 'S138', 'S149', 'S154', 'S165', 'S168', 'S171', 'S178']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_list,val_list,test_list = subject_split(S_cnt = S_cnt,using_num=[0,1] ,seed = 7,print_list=True)\n",
    "X_train,y_train,X_val,y_val,X_test,y_test = preprocessing(train_list,val_list,test_list,is_scale=True,e_num=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((869, 32, 32, 63),\n",
       " (93, 32, 32, 63),\n",
       " (279, 32, 32, 63),\n",
       " (869, 2),\n",
       " (93, 2),\n",
       " (279, 2))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_val.shape, X_test.shape,y_train.shape, y_val.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D,GlobalAveragePooling2D\n",
    "def cnn_model(input_shape, drop_rate=0.5, nb_classes=2):\n",
    "    model=Sequential()\n",
    "    \n",
    "    # Layer 1\n",
    "    model.add(Conv2D (kernel_size=3, filters=256, strides=1, padding='same',\n",
    "                      kernel_initializer='he_uniform', input_shape=input_shape))                  \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    # Layer 2\n",
    "    model.add(Conv2D(kernel_size=3, filters=256, strides=1, padding='same', kernel_initializer='he_uniform'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    # Layer 3\n",
    "    model.add(Conv2D (kernel_size=3, filters=256, strides=2, padding='same', kernel_initializer='he_uniform'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "#     model.add(MaxPooling2D(pool_size=2, strides=2))\n",
    "    \n",
    "    # Layer 3\n",
    "    model.add(Conv2D (kernel_size=3, filters=256, strides=2, padding='same', kernel_initializer='he_uniform'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "#     model.add(MaxPooling2D(pool_size=2, strides=2))\n",
    "    \n",
    "    # Layer 3\n",
    "    model.add(Conv2D (kernel_size=3, filters=256, strides=2, padding='same', kernel_initializer='he_uniform'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "#     model.add(MaxPooling2D(pool_size=2, strides=2))\n",
    "    \n",
    "    # Layer 3\n",
    "    model.add(Conv2D (kernel_size=3, filters=256, strides=2, padding='same', kernel_initializer='he_uniform'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    # Layer 11\n",
    "    model.add(Dropout(drop_rate))\n",
    "    model.add(GlobalAveragePooling2D())\n",
    "\n",
    "    # Layer 12\n",
    "    model.add(Dense(nb_classes))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test result visulization\n",
    "# def test(model,weight_path,X_test,y_test):\n",
    "#     model = model\n",
    "#     model.load_weights(weigth_file)\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "#         print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "#         print('Confusion matrix, without normalization')\n",
    "        pass\n",
    "#     print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_shot(e_num = 3):\n",
    "    batch_size = 16\n",
    "    epochs = 100\n",
    "    input_shape = (224,63)\n",
    "    output_size = 2\n",
    "    \n",
    "    \n",
    "    \n",
    "    now = datetime.now()\n",
    "    nowDate = now.strftime('%d_%H_%M_%S')\n",
    "    model_name = 'my_model'+ nowDate\n",
    "    \n",
    "    \n",
    "#     model = get_model5(input_shape = (224,63),nb_classes = 2)\n",
    "    model = cnn_model(input_shape = (32,32,63),nb_classes = 2)\n",
    "#     model = MyModel(training=True)\n",
    "#     model = conv2d(input_shape = (16,16,63),nb_classes = 2)\n",
    "    \n",
    "#     model = cnn_model(input_shape=input_shape, output_size=output_size)\n",
    "    model.summary()\n",
    "    \n",
    "    model = multi_gpu_model(model,gpus=2)\n",
    "    model.summary()\n",
    "    model.compile(loss=tf.keras.losses.binary_crossentropy, optimizer='sgd')\n",
    "    \n",
    "    # Validation 점수가 가장 좋은 모델만 저장합니다.\n",
    "    checkpoint_path = os.path.join(model_path, model_name)\n",
    "    os.makedirs(checkpoint_path, exist_ok=True)\n",
    "    model_file_path = os.path.join(checkpoint_path, 'Epoch_{epoch:03d}_Val_{val_loss:.3f}.hdf5')\n",
    "    checkpoint = ModelCheckpoint(filepath=model_file_path, monitor='val_loss', verbose=1, save_best_only=True)\n",
    "\n",
    "    # 10회 간 Validation 점수가 좋아지지 않으면 중지합니다.\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "    \n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                              patience=5, min_lr=0.001)\n",
    "    \n",
    "    train_list,val_list,test_list = subject_split(S_cnt = S_cnt,using_num=[0,1],print_list=True)\n",
    "    X_train,y_train,X_val,y_val,X_test,y_test = preprocessing(train_list,val_list,test_list,is_scale=True,e_num=e_num)\n",
    "    \n",
    "    \n",
    "    print('train len : ', np.shape(X_train),np.shape(y_train))\n",
    "    print('val len : ', np.shape(X_val),np.shape(y_val))\n",
    "    print('test len : ', np.shape(X_test),np.shape(y_test))\n",
    "    history = model.fit(\n",
    "    X_train, y_train, validation_data=(X_val,y_val),\n",
    "    epochs=epochs, batch_size=batch_size,  shuffle=True,\n",
    "    callbacks=[checkpoint, early_stopping])\n",
    "#     X_train, X_train, validation_data=(X_val,X_val),\n",
    "#     epochs=epochs, batch_size=batch_size,  shuffle=True,\n",
    "#     callbacks=[checkpoint, early_stopping,reduce_lr])    \n",
    "    \n",
    "        \n",
    "    plt.plot(history.epoch, history.history['loss'], '-o', label='training_loss')\n",
    "    plt.plot(history.epoch, history.history['val_loss'], '-o', label='validation_loss')\n",
    "    plt.legend()\n",
    "    plt.xlim(left=0)\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('loss')\n",
    "    plt.show()\n",
    "        \n",
    "    checkpoint_path = os.path.join(model_path, model_name)\n",
    "    print(checkpoint_path)\n",
    "    weigth_files = glob.glob('{}/*.hdf5'.format(checkpoint_path))\n",
    "    weigth_files.sort()\n",
    "    weigth_file = weigth_files[-1]\n",
    "    print(weigth_files)\n",
    "    print(weigth_file)\n",
    "    model.load_weights(weigth_file)\n",
    "    \n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "#     print(y_pred)\n",
    "#     print(y_test)\n",
    "    y_true,y_pred = np.argmax(y_test,axis=-1),np.argmax(y_pred,axis=-1)\n",
    "    test_acc = accuracy_score(y_true,y_pred)\n",
    "    cnf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    plot_confusion_matrix(cnf_matrix,['N','MCI'],title='model : {} , test_acc : {}'.format(model_name,test_acc))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 32, 32, 256)       145408    \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 32, 32, 256)       1024      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 256)       590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 32, 32, 256)       1024      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 16, 16, 256)       590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 16, 16, 256)       1024      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 8, 8, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 4, 4, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 4, 4, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 2, 2, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 2, 2, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 514       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 3,102,466\n",
      "Trainable params: 3,099,394\n",
      "Non-trainable params: 3,072\n",
      "_________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "conv2d_input (InputLayer)       (None, 32, 32, 63)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 32, 32, 63)   0           conv2d_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 32, 32, 63)   0           conv2d_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "sequential (Sequential)         (None, 2)            3102466     lambda[0][0]                     \n",
      "                                                                 lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Concatenate)      (None, 2)            0           sequential[1][0]                 \n",
      "                                                                 sequential[2][0]                 \n",
      "==================================================================================================\n",
      "Total params: 3,102,466\n",
      "Trainable params: 3,099,394\n",
      "Non-trainable params: 3,072\n",
      "__________________________________________________________________________________________________\n",
      "label 0 split result = train : 66 , val : 8 , test : 21 : \n",
      "label 1 split result = train : 48 , val : 6 , test : 15 : \n",
      "total split result = train : 114 , val : 14 , test : 36\n",
      "\n",
      "train_list\n",
      " ['S001', 'S002', 'S003', 'S004', 'S007', 'S008', 'S010', 'S018', 'S019', 'S021', 'S023', 'S024', 'S026', 'S027', 'S029', 'S030', 'S031', 'S032', 'S034', 'S036', 'S037', 'S039', 'S040', 'S041', 'S046', 'S047', 'S049', 'S051', 'S053', 'S054', 'S055', 'S056', 'S057', 'S058', 'S059', 'S060', 'S063', 'S064', 'S065', 'S068', 'S069', 'S070', 'S072', 'S073', 'S075', 'S076', 'S077', 'S078', 'S080', 'S081', 'S085', 'S087', 'S088', 'S090', 'S092', 'S093', 'S094', 'S095', 'S096', 'S098', 'S099', 'S101', 'S102', 'S104', 'S105', 'S108', 'S109', 'S110', 'S111', 'S113', 'S114', 'S115', 'S119', 'S122', 'S125', 'S126', 'S127', 'S129', 'S132', 'S133', 'S136', 'S137', 'S139', 'S141', 'S143', 'S144', 'S145', 'S146', 'S148', 'S150', 'S151', 'S152', 'S153', 'S154', 'S155', 'S157', 'S158', 'S159', 'S160', 'S161', 'S162', 'S165', 'S167', 'S168', 'S172', 'S174', 'S175', 'S176', 'S177', 'S178', 'S183', 'S187', 'S191', 'S192']\n",
      "\n",
      "val_list\n",
      " ['S011', 'S017', 'S028', 'S043', 'S045', 'S062', 'S079', 'S082', 'S103', 'S120', 'S124', 'S134', 'S147', 'S190']\n",
      "\n",
      "test_list\n",
      " ['S005', 'S009', 'S020', 'S033', 'S035', 'S038', 'S042', 'S048', 'S050', 'S061', 'S083', 'S084', 'S086', 'S089', 'S091', 'S097', 'S100', 'S112', 'S116', 'S117', 'S118', 'S121', 'S123', 'S128', 'S130', 'S131', 'S135', 'S138', 'S140', 'S149', 'S163', 'S164', 'S170', 'S171', 'S173', 'S184']\n",
      "train_list data load\n",
      "val_list data load\n",
      "['S011', 'S017', 'S028', 'S043', 'S045', 'S062', 'S079', 'S082', 'S103', 'S120', 'S124', 'S134', 'S147', 'S190']\n",
      "test_list data load\n",
      "['S005', 'S009', 'S020', 'S033', 'S035', 'S038', 'S042', 'S048', 'S050', 'S061', 'S083', 'S084', 'S086', 'S089', 'S091', 'S097', 'S100', 'S112', 'S116', 'S117', 'S118', 'S121', 'S123', 'S128', 'S130', 'S131', 'S135', 'S138', 'S140', 'S149', 'S163', 'S164', 'S170', 'S171', 'S173', 'S184']\n",
      "train len :  (332, 32, 32, 63) (332, 2)\n",
      "val len :  (40, 32, 32, 63) (40, 2)\n",
      "test len :  (84, 32, 32, 63) (84, 2)\n",
      "Train on 332 samples, validate on 40 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[{{node conv2d/Conv2D}} = Conv2D[T=DT_FLOAT, data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](conv2d/Conv2D-0-TransposeNHWCToNCHW-LayoutOptimizer, conv2d/Conv2D/ReadVariableOp)]]\n\t [[{{node loss/activation_6_loss/broadcast_weights/assert_broadcastable/AssertGuard/Assert/Switch/_337}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_1909_...ert/Switch\", tensor_type=DT_BOOL, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-501ef51cda38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mone_shot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-1bbfaa683f30>\u001b[0m in \u001b[0;36mone_shot\u001b[0;34m(e_num)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     callbacks=[checkpoint, early_stopping])\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;31m#     X_train, X_train, validation_data=(X_val,X_val),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m#     epochs=epochs, batch_size=batch_size,  shuffle=True,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/jyh/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1637\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1638\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1639\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1641\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/jyh/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    213\u001b[0m           \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m           \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/jyh/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2984\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2985\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 2986\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   2987\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2988\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/jyh/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/jyh/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    529\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnknownError\u001b[0m: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[{{node conv2d/Conv2D}} = Conv2D[T=DT_FLOAT, data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](conv2d/Conv2D-0-TransposeNHWCToNCHW-LayoutOptimizer, conv2d/Conv2D/ReadVariableOp)]]\n\t [[{{node loss/activation_6_loss/broadcast_weights/assert_broadcastable/AssertGuard/Assert/Switch/_337}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_1909_...ert/Switch\", tensor_type=DT_BOOL, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]"
     ]
    }
   ],
   "source": [
    "one_shot(e_num=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_shot(e_num=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_shot(e_num=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_shot(e_num=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(869, 16, 2, 63)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:,:,[0,1]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.6",
   "language": "python",
   "name": "jyh"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
